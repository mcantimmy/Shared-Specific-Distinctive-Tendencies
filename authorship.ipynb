{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 23.9k/23.9k [00:00<00:00, 24.0MB/s]\n",
      "Downloading data: 100%|██████████| 3.78M/3.78M [00:01<00:00, 3.34MB/s]\n",
      "Downloading data: 100%|██████████| 901k/901k [00:00<00:00, 1.03MB/s]\n",
      "Downloading data: 100%|██████████| 167k/167k [00:00<00:00, 751kB/s]\n",
      "Generating train split: 100%|██████████| 45615/45615 [00:00<00:00, 611435.88 examples/s]\n",
      "Generating test split: 100%|██████████| 12284/12284 [00:00<00:00, 1501116.75 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 630390.62 examples/s]\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mcant\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 2895/2895 [3:37:06<00:00,  4.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.6622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 724/724 [18:42<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7161\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.63      0.67      2258\n",
      "           1       0.75      0.64      0.69      5322\n",
      "           2       0.68      0.86      0.76      4000\n",
      "\n",
      "    accuracy                           0.72     11580\n",
      "   macro avg       0.72      0.71      0.71     11580\n",
      "weighted avg       0.72      0.72      0.71     11580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2895/2895 [3:32:49<00:00,  4.41s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.4794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 724/724 [18:41<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.62      0.66      2258\n",
      "           1       0.69      0.79      0.74      5322\n",
      "           2       0.80      0.71      0.75      4000\n",
      "\n",
      "    accuracy                           0.73     11580\n",
      "   macro avg       0.73      0.70      0.72     11580\n",
      "weighted avg       0.73      0.73      0.73     11580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2895/2895 [3:35:18<00:00,  4.46s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 724/724 [18:45<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7133\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.72      0.68      2258\n",
      "           1       0.70      0.72      0.71      5322\n",
      "           2       0.78      0.70      0.74      4000\n",
      "\n",
      "    accuracy                           0.71     11580\n",
      "   macro avg       0.71      0.71      0.71     11580\n",
      "weighted avg       0.72      0.71      0.71     11580\n",
      "\n",
      "Training completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the TweetEval dataset\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "# Prepare the data\n",
    "texts = dataset[\"train\"][\"text\"] + dataset[\"test\"][\"text\"]\n",
    "labels = dataset[\"train\"][\"label\"] + dataset[\"test\"][\"label\"]\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TweetDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TweetDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(classification_report(val_true, val_preds))\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"tweet_authorship_model\")\n",
    "tokenizer.save_pretrained(\"tweet_authorship_model\")\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mcant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mcant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  8%|▊         | 10/118 [00:50<08:17,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8421, 'grad_norm': 6.978228569030762, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20/118 [01:35<07:22,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.858, 'grad_norm': 6.721885681152344, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 30/118 [02:21<06:32,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8734, 'grad_norm': 5.634627342224121, 'learning_rate': 3e-06, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 40/118 [03:07<05:52,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8622, 'grad_norm': 5.481440544128418, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 50/118 [03:53<05:12,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8103, 'grad_norm': 6.180174350738525, 'learning_rate': 5e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 60/118 [04:36<03:48,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.788, 'grad_norm': 8.272601127624512, 'learning_rate': 6e-06, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 70/118 [05:22<03:37,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7908, 'grad_norm': 9.912836074829102, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 80/118 [06:08<02:53,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7754, 'grad_norm': 7.177762031555176, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 90/118 [06:53<02:06,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8321, 'grad_norm': 5.76736307144165, 'learning_rate': 9e-06, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 100/118 [07:39<01:21,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.789, 'grad_norm': 6.290131568908691, 'learning_rate': 1e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 110/118 [08:24<00:36,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7866, 'grad_norm': 8.413372039794922, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [08:58<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 538.1207, 'train_samples_per_second': 3.464, 'train_steps_per_second': 0.219, 'train_loss': 4.816403114189536, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:20<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03424657534246575\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 114, does not match size of target_names, 123. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(test_labels, preds))\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2626\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2620\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2621\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2622\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2623\u001b[0m             )\n\u001b[0;32m   2624\u001b[0m         )\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2626\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2627\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2630\u001b[0m         )\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2632\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 114, does not match size of target_names, 123. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets torch scikit-learn pandas nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the Reddit dataset\n",
    "dataset = load_dataset(\"reddit\", split=\"train[:100000]\", trust_remote_code=True)  # Limiting to 100k samples for this example\n",
    "\n",
    "# Convert to pandas DataFrame for easier preprocessing\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['content']\n",
    "\n",
    "# Use 'author' as our target for authorship attribution\n",
    "# Keep only authors with at least 50 comments\n",
    "author_counts = df['author'].value_counts()\n",
    "authors_to_keep = author_counts[author_counts >= 8].index\n",
    "df = df[df['author'].isin(authors_to_keep)]\n",
    "\n",
    "# Encode author labels\n",
    "le = LabelEncoder()\n",
    "df['author_encoded'] = le.fit_transform(df['author'])\n",
    "\n",
    "# When splitting the data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['processed_text'], df['author_encoded'], \n",
    "    test_size=0.2, random_state=42, stratify=df['author_encoded']\n",
    ")\n",
    "\n",
    "# Further split train into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, \n",
    "    test_size=0.1, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "val_encodings = tokenize_function(val_texts.tolist())\n",
    "test_encodings = tokenize_function(test_texts.tolist())\n",
    "\n",
    "# Dataset class\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RedditDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = RedditDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = RedditDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(le.classes_))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(accuracy_score(test_labels, preds))\n",
    "print(classification_report(test_labels, preds, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:18<00:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03424657534246575\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "      A_Polite_Noise       0.00      0.00      0.00         3\n",
      "           Anomander       0.00      0.00      0.00         3\n",
      "            BZenMojo       0.00      0.00      0.00         1\n",
      "          Batty-Koda       0.00      0.00      0.00         5\n",
      "       Blenderhead36       0.00      0.00      0.00         2\n",
      "   BluepillProfessor       0.00      0.00      0.00         1\n",
      "   BuildMyPaperHeart       0.00      0.00      0.00         1\n",
      "           CaspianX2       0.00      0.00      0.00         2\n",
      "     Cebus_capucinus       0.00      0.00      0.00         1\n",
      "             Chaipod       0.00      0.00      0.00         3\n",
      "          CocoSavege       0.00      0.00      0.00         4\n",
      "        DashingLeech       0.00      0.00      0.00         3\n",
      "         Death_Star_       0.00      0.00      0.00         4\n",
      "             DejaBoo       0.04      0.10      0.06        10\n",
      "             DesCo83       0.00      0.00      0.00         1\n",
      "Dickcheese_McDoogles       0.00      0.00      0.00         4\n",
      "             EnzoYug       0.00      0.00      0.00         3\n",
      "           FalconOne       0.00      0.00      0.00         3\n",
      "    Fearlessleader85       0.00      0.00      0.00         2\n",
      "          FrankManic       0.00      0.00      0.00         1\n",
      "      HeathenCyclist       0.00      0.00      0.00         6\n",
      "        Hiddencamper       0.00      0.00      0.00         1\n",
      "        InternetFree       0.00      0.00      0.00         2\n",
      "      IrrelevantTLDR       0.00      0.00      0.00         7\n",
      "              Kalium       0.00      0.00      0.00         4\n",
      "                KoNP       0.08      0.33      0.12         3\n",
      "        LarrySDonald       0.00      0.00      0.00         2\n",
      "      Lord_NiteShade       0.00      0.00      0.00         2\n",
      "          MathPolice       0.00      0.00      0.00         1\n",
      "    MegalomaniacHack       0.00      0.00      0.00         4\n",
      "               Monso       0.00      0.00      0.00         1\n",
      "          Mzsickness       0.00      0.00      0.00         2\n",
      "     Nightmathzombie       0.00      0.00      0.00         2\n",
      "         NukeThePope       0.00      0.00      0.00         6\n",
      "            Osiris32       0.00      0.00      0.00         2\n",
      "  Philo_T_Farnsworth       0.00      0.00      0.00         3\n",
      "     RamsesThePigeon       0.50      0.33      0.40         6\n",
      "    Rancid_Bear_Meat       0.00      0.00      0.00         1\n",
      "      RedditAntiHero       0.00      0.00      0.00         2\n",
      "      SearingPhoenix       0.00      0.00      0.00         1\n",
      "        ShakoraDrake       0.00      0.00      0.00         1\n",
      "          Shaper_pmp       0.12      0.10      0.11        10\n",
      "          ShortWoman       0.00      0.00      0.00         1\n",
      "          Stingray88       0.00      0.00      0.00         2\n",
      "  TheRealAlfredAdler       0.00      0.00      0.00         1\n",
      "TheRufmeisterGeneral       0.00      0.00      0.00         3\n",
      "        ThreeTimesUp       0.00      0.00      0.00         2\n",
      "     ThrustVectoring       0.00      0.00      0.00         3\n",
      "     TooManyInLitter       0.00      0.00      0.00         3\n",
      "          Viperbunny       0.00      0.00      0.00         1\n",
      "            WalkerEU       0.00      0.00      0.00         2\n",
      "        ZeroNihilist       0.02      0.50      0.04         2\n",
      "            Zircon88       0.00      0.00      0.00         2\n",
      "            adelie42       0.00      0.00      0.00         3\n",
      "          adrianmonk       0.00      0.00      0.00         2\n",
      "            afcagroo       0.00      0.00      0.00         4\n",
      "          ajsdklf9df       0.00      0.00      0.00         1\n",
      "          alexisaacs       0.00      0.00      0.00         1\n",
      "            andytuba       0.00      0.00      0.00         3\n",
      "          angelworks       0.00      0.00      0.00         3\n",
      "            ashole11       0.00      0.00      0.00         2\n",
      "    ass_munch_reborn       0.00      0.00      0.00         2\n",
      "             avapoet       0.00      0.00      0.00         3\n",
      "         bmcclure937       0.00      0.00      0.00         1\n",
      "         branewalker       0.00      0.00      0.00         4\n",
      "           cabbagery       0.00      0.00      0.00         4\n",
      "         chadillac83       0.00      0.00      0.00         3\n",
      "             codayus       0.00      0.00      0.00         5\n",
      "          devedander       0.00      0.00      0.00         3\n",
      "      dinosaur_train       0.00      0.00      0.00         1\n",
      "      doesurmindglow       0.00      0.00      0.00         1\n",
      "        droidblaster       0.00      0.00      0.00         1\n",
      "              dsprox       0.00      0.00      0.00         1\n",
      "              dumboy       0.00      0.00      0.00         2\n",
      "         emeraldrumm       0.00      0.00      0.00         2\n",
      "           eroverton       0.00      0.00      0.00         1\n",
      "             feureau       0.00      0.00      0.00         3\n",
      "       freedomweasel       0.00      0.00      0.00         1\n",
      "       gingerkid1234       0.00      0.00      0.00         4\n",
      "  greenRiverThriller       0.00      0.00      0.00         3\n",
      "      hardtoremember       0.00      0.00      0.00         1\n",
      "         herman_gill       0.00      0.00      0.00         4\n",
      "      iamadogforreal       0.00      0.00      0.00         1\n",
      "        josiahpapaya       0.00      0.00      0.00         2\n",
      "           kibbleh21       0.00      0.00      0.00         4\n",
      "              kuvter       0.05      0.60      0.08         5\n",
      "           laterdude       0.00      0.00      0.00         2\n",
      "        lumpy_potato       0.00      0.00      0.00         3\n",
      "              mauxly       0.00      0.00      0.00         3\n",
      "            megatom0       0.00      0.00      0.00         2\n",
      "            misnamed       0.00      0.00      0.00         4\n",
      "         mynameipaul       0.00      0.00      0.00         3\n",
      "              myztry       0.00      0.00      0.00         4\n",
      "           nicholsml       0.00      0.00      0.00         3\n",
      "          nuclearwar       0.00      0.00      0.00         1\n",
      "       parlor_tricks       0.00      0.00      0.00         3\n",
      "          pixis-4950       0.00      0.00      0.00         1\n",
      "    probablydyslexic       0.01      0.50      0.02         2\n",
      "             rand486       0.00      0.00      0.00         4\n",
      "           redweasel       0.00      0.00      0.00         3\n",
      "        rivalarrival       0.00      0.00      0.00         1\n",
      "           robotevil       0.00      0.00      0.00         1\n",
      "        rogersmith25       0.00      0.00      0.00         1\n",
      "           shevagleb       0.00      0.00      0.00         2\n",
      "             skpkzk2       0.00      0.00      0.00         2\n",
      "            smacksaw       0.00      0.00      0.00         3\n",
      "              tubcat       0.00      0.00      0.00         3\n",
      "               twwwy       0.00      0.00      0.00         3\n",
      "      veni_vidi_vale       0.00      0.00      0.00         2\n",
      "            viramola       0.00      0.00      0.00         1\n",
      "          well_golly       0.00      0.00      0.00         1\n",
      " what_a_cat_astrophe       0.00      0.00      0.00         2\n",
      "        wonderfuldog       0.00      0.00      0.00         2\n",
      "         ychromosome       0.00      0.00      0.00         1\n",
      "\n",
      "            accuracy                           0.03       292\n",
      "           macro avg       0.01      0.02      0.01       292\n",
      "        weighted avg       0.02      0.03      0.02       292\n",
      "\n",
      "Authors in test set:\n",
      "Label 1: A_Polite_Noise\n",
      "Label 2: Anomander\n",
      "Label 3: BZenMojo\n",
      "Label 4: Batty-Koda\n",
      "Label 5: Blenderhead36\n",
      "Label 6: BluepillProfessor\n",
      "Label 7: BuildMyPaperHeart\n",
      "Label 8: CaspianX2\n",
      "Label 9: Cebus_capucinus\n",
      "Label 10: Chaipod\n",
      "Label 11: CocoSavege\n",
      "Label 12: DashingLeech\n",
      "Label 13: Death_Star_\n",
      "Label 14: DejaBoo\n",
      "Label 15: DesCo83\n",
      "Label 16: Dickcheese_McDoogles\n",
      "Label 17: EnzoYug\n",
      "Label 18: FalconOne\n",
      "Label 19: Fearlessleader85\n",
      "Label 21: FrankManic\n",
      "Label 22: HeathenCyclist\n",
      "Label 23: Hiddencamper\n",
      "Label 24: InternetFree\n",
      "Label 25: IrrelevantTLDR\n",
      "Label 26: Kalium\n",
      "Label 27: KoNP\n",
      "Label 28: LarrySDonald\n",
      "Label 29: Lord_NiteShade\n",
      "Label 31: MathPolice\n",
      "Label 32: MegalomaniacHack\n",
      "Label 33: Monso\n",
      "Label 34: Mzsickness\n",
      "Label 35: Nightmathzombie\n",
      "Label 36: NukeThePope\n",
      "Label 37: Osiris32\n",
      "Label 38: Philo_T_Farnsworth\n",
      "Label 39: RamsesThePigeon\n",
      "Label 40: Rancid_Bear_Meat\n",
      "Label 41: RedditAntiHero\n",
      "Label 42: SearingPhoenix\n",
      "Label 43: ShakoraDrake\n",
      "Label 44: Shaper_pmp\n",
      "Label 45: ShortWoman\n",
      "Label 46: Stingray88\n",
      "Label 47: TheRealAlfredAdler\n",
      "Label 48: TheRufmeisterGeneral\n",
      "Label 49: ThreeTimesUp\n",
      "Label 50: ThrustVectoring\n",
      "Label 51: TooManyInLitter\n",
      "Label 52: Viperbunny\n",
      "Label 53: WalkerEU\n",
      "Label 54: ZeroNihilist\n",
      "Label 55: Zircon88\n",
      "Label 56: adelie42\n",
      "Label 57: adrianmonk\n",
      "Label 58: afcagroo\n",
      "Label 59: ajsdklf9df\n",
      "Label 60: alexisaacs\n",
      "Label 61: andytuba\n",
      "Label 62: angelworks\n",
      "Label 63: ashole11\n",
      "Label 64: ass_munch_reborn\n",
      "Label 65: avapoet\n",
      "Label 66: bmcclure937\n",
      "Label 67: branewalker\n",
      "Label 68: cabbagery\n",
      "Label 69: chadillac83\n",
      "Label 71: codayus\n",
      "Label 72: devedander\n",
      "Label 73: dinosaur_train\n",
      "Label 74: doesurmindglow\n",
      "Label 75: droidblaster\n",
      "Label 76: dsprox\n",
      "Label 77: dumboy\n",
      "Label 78: emeraldrumm\n",
      "Label 79: eroverton\n",
      "Label 80: feureau\n",
      "Label 81: freedomweasel\n",
      "Label 82: gingerkid1234\n",
      "Label 83: greenRiverThriller\n",
      "Label 84: hardtoremember\n",
      "Label 85: herman_gill\n",
      "Label 86: iamadogforreal\n",
      "Label 87: josiahpapaya\n",
      "Label 88: kibbleh21\n",
      "Label 89: kuvter\n",
      "Label 90: laterdude\n",
      "Label 91: lumpy_potato\n",
      "Label 92: mauxly\n",
      "Label 93: megatom0\n",
      "Label 94: misnamed\n",
      "Label 96: mynameipaul\n",
      "Label 97: myztry\n",
      "Label 99: nicholsml\n",
      "Label 101: nuclearwar\n",
      "Label 102: parlor_tricks\n",
      "Label 103: pixis-4950\n",
      "Label 104: probablydyslexic\n",
      "Label 105: rand486\n",
      "Label 106: redweasel\n",
      "Label 107: rivalarrival\n",
      "Label 108: robotevil\n",
      "Label 109: rogersmith25\n",
      "Label 110: shevagleb\n",
      "Label 111: skpkzk2\n",
      "Label 112: smacksaw\n",
      "Label 114: tubcat\n",
      "Label 115: twwwy\n",
      "Label 117: veni_vidi_vale\n",
      "Label 118: viramola\n",
      "Label 119: well_golly\n",
      "Label 120: what_a_cat_astrophe\n",
      "Label 121: wonderfuldog\n",
      "Label 122: ychromosome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# After making predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Get the unique classes in our test set\n",
    "unique_classes = np.unique(test_labels)\n",
    "\n",
    "# Create a mapping from the original label encoder to the classes in our test set\n",
    "label_map = {i: le.classes_[i] for i in unique_classes}\n",
    "\n",
    "# Generate the classification report\n",
    "print(accuracy_score(test_labels, preds))\n",
    "print(classification_report(test_labels, preds, \n",
    "                            target_names=[label_map[i] for i in sorted(label_map.keys())],\n",
    "                            labels=sorted(label_map.keys())))\n",
    "\n",
    "# If you want to see which authors are in the test set\n",
    "print(\"Authors in test set:\")\n",
    "for i, author in label_map.items():\n",
    "    print(f\"Label {i}: {author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Replace the BERT tokenizer with RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# ... (rest of the data preparation code remains the same)\n",
    "\n",
    "# Replace the BERT model with RoBERTa\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=len(le.classes_))\n",
    "\n",
    "# Adjust training arguments for the larger model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,  # You might need fewer epochs with a more powerful model\n",
    "    per_device_train_batch_size=8,  # Reduced batch size due to larger model\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2,  # This effectively doubles the batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier preprocessing\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_text'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df['processed_text'] = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  2%|▏         | 2/132 [00:15<16:58,  7.83s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use 'author' as our target for authorship attribution\n",
    "# Keep only authors with at least 50 comments\n",
    "author_counts = df['author'].value_counts()\n",
    "authors_to_keep = author_counts[author_counts >= 8].index\n",
    "df = df[df['author'].isin(authors_to_keep)]\n",
    "\n",
    "# Encode author labels\n",
    "le = LabelEncoder()\n",
    "df['author_encoded'] = le.fit_transform(df['author'])\n",
    "\n",
    "# Split the data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['processed_text'], df['author_encoded'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Further split train into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "val_encodings = tokenize_function(val_texts.tolist())\n",
    "test_encodings = tokenize_function(test_texts.tolist())\n",
    "\n",
    "# Dataset class\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RedditDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = RedditDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = RedditDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(le.classes_))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(accuracy_score(test_labels, preds))\n",
    "print(classification_report(test_labels, preds, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mcant\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▌         | 10/174 [03:06<45:49, 16.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8226, 'grad_norm': 12.095752716064453, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 20/174 [05:49<41:04, 16.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8732, 'grad_norm': 39.97496032714844, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 30/174 [08:27<37:42, 15.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8165, 'grad_norm': 9.188164710998535, 'learning_rate': 3e-06, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 40/174 [11:04<35:05, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8256, 'grad_norm': 10.712109565734863, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 50/174 [13:43<32:26, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8151, 'grad_norm': 15.533021926879883, 'learning_rate': 5e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 60/174 [16:17<28:20, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.8367, 'grad_norm': 16.344280242919922, 'learning_rate': 6e-06, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 70/174 [18:56<27:27, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7927, 'grad_norm': 280.9397888183594, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 80/174 [21:32<24:25, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7472, 'grad_norm': 10.857053756713867, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 90/174 [24:08<21:43, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7497, 'grad_norm': 14.414872169494629, 'learning_rate': 9e-06, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 100/174 [26:44<19:08, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7277, 'grad_norm': 29.80389976501465, 'learning_rate': 1e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 110/174 [29:22<17:01, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.7703, 'grad_norm': 13.208962440490723, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 120/174 [31:58<14:03, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6662, 'grad_norm': 15.258040428161621, 'learning_rate': 1.2e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 130/174 [34:37<11:47, 16.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5175, 'grad_norm': 15.18865966796875, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 140/174 [37:19<09:11, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5451, 'grad_norm': 17.078166961669922, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 150/174 [39:56<06:14, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5563, 'grad_norm': 19.71668243408203, 'learning_rate': 1.5e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 160/174 [42:37<03:45, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4806, 'grad_norm': 43.998023986816406, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 170/174 [45:14<01:03, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4882, 'grad_norm': 17.25783920288086, 'learning_rate': 1.7000000000000003e-05, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174/174 [46:16<00:00, 15.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2776.6647, 'train_samples_per_second': 1.007, 'train_steps_per_second': 0.063, 'train_loss': 4.7029325989471085, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [01:23<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11301369863013698\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "      A_Polite_Noise       0.00      0.00      0.00         3\n",
      "           Anomander       0.00      0.00      0.00         3\n",
      "            BZenMojo       0.00      0.00      0.00         1\n",
      "          Batty-Koda       0.00      0.00      0.00         5\n",
      "       Blenderhead36       0.00      0.00      0.00         2\n",
      "   BluepillProfessor       0.00      0.00      0.00         1\n",
      "   BuildMyPaperHeart       0.00      0.00      0.00         1\n",
      "           CaspianX2       0.00      0.00      0.00         2\n",
      "     Cebus_capucinus       0.00      0.00      0.00         1\n",
      "             Chaipod       0.50      0.33      0.40         3\n",
      "          CocoSavege       0.00      0.00      0.00         4\n",
      "        DashingLeech       0.00      0.00      0.00         3\n",
      "         Death_Star_       0.00      0.00      0.00         4\n",
      "             DejaBoo       0.18      0.90      0.30        10\n",
      "             DesCo83       0.00      0.00      0.00         1\n",
      "Dickcheese_McDoogles       0.80      1.00      0.89         4\n",
      "             EnzoYug       0.00      0.00      0.00         3\n",
      "           FalconOne       0.00      0.00      0.00         3\n",
      "    Fearlessleader85       0.00      0.00      0.00         2\n",
      "          FrankManic       0.00      0.00      0.00         1\n",
      "      HeathenCyclist       0.00      0.00      0.00         6\n",
      "        Hiddencamper       1.00      1.00      1.00         1\n",
      "        InternetFree       0.00      0.00      0.00         2\n",
      "      IrrelevantTLDR       0.07      0.14      0.09         7\n",
      "              Kalium       0.00      0.00      0.00         4\n",
      "                KoNP       0.00      0.00      0.00         3\n",
      "        LarrySDonald       0.00      0.00      0.00         2\n",
      "      Lord_NiteShade       0.00      0.00      0.00         2\n",
      "          MathPolice       0.00      0.00      0.00         1\n",
      "    MegalomaniacHack       0.00      0.00      0.00         4\n",
      "               Monso       0.00      0.00      0.00         1\n",
      "          Mzsickness       0.00      0.00      0.00         2\n",
      "     Nightmathzombie       0.00      0.00      0.00         2\n",
      "         NukeThePope       0.00      0.00      0.00         6\n",
      "            Osiris32       0.00      0.00      0.00         2\n",
      "  Philo_T_Farnsworth       0.00      0.00      0.00         3\n",
      "     RamsesThePigeon       0.04      0.50      0.07         6\n",
      "    Rancid_Bear_Meat       0.00      0.00      0.00         1\n",
      "      RedditAntiHero       0.00      0.00      0.00         2\n",
      "      SearingPhoenix       0.00      0.00      0.00         1\n",
      "        ShakoraDrake       0.00      0.00      0.00         1\n",
      "          Shaper_pmp       0.14      0.20      0.17        10\n",
      "          ShortWoman       0.00      0.00      0.00         1\n",
      "          Stingray88       0.00      0.00      0.00         2\n",
      "  TheRealAlfredAdler       0.00      0.00      0.00         1\n",
      "TheRufmeisterGeneral       0.00      0.00      0.00         3\n",
      "        ThreeTimesUp       1.00      0.50      0.67         2\n",
      "     ThrustVectoring       0.00      0.00      0.00         3\n",
      "     TooManyInLitter       0.00      0.00      0.00         3\n",
      "          Viperbunny       0.00      0.00      0.00         1\n",
      "            WalkerEU       0.00      0.00      0.00         2\n",
      "        ZeroNihilist       0.00      0.00      0.00         2\n",
      "            Zircon88       0.00      0.00      0.00         2\n",
      "            adelie42       0.00      0.00      0.00         3\n",
      "          adrianmonk       0.00      0.00      0.00         2\n",
      "            afcagroo       0.00      0.00      0.00         4\n",
      "          ajsdklf9df       0.00      0.00      0.00         1\n",
      "          alexisaacs       0.00      0.00      0.00         1\n",
      "            andytuba       0.00      0.00      0.00         3\n",
      "          angelworks       0.00      0.00      0.00         3\n",
      "            ashole11       0.00      0.00      0.00         2\n",
      "    ass_munch_reborn       0.00      0.00      0.00         2\n",
      "             avapoet       0.00      0.00      0.00         3\n",
      "         bmcclure937       0.00      0.00      0.00         1\n",
      "         branewalker       0.00      0.00      0.00         4\n",
      "           cabbagery       0.00      0.00      0.00         4\n",
      "         chadillac83       0.00      0.00      0.00         3\n",
      "             codayus       0.12      1.00      0.22         5\n",
      "          devedander       0.00      0.00      0.00         3\n",
      "      dinosaur_train       0.00      0.00      0.00         1\n",
      "      doesurmindglow       0.00      0.00      0.00         1\n",
      "        droidblaster       0.14      1.00      0.25         1\n",
      "              dsprox       0.00      0.00      0.00         1\n",
      "              dumboy       0.00      0.00      0.00         2\n",
      "         emeraldrumm       0.00      0.00      0.00         2\n",
      "           eroverton       0.00      0.00      0.00         1\n",
      "             feureau       0.00      0.00      0.00         3\n",
      "       freedomweasel       0.00      0.00      0.00         1\n",
      "       gingerkid1234       0.00      0.00      0.00         4\n",
      "  greenRiverThriller       0.00      0.00      0.00         3\n",
      "      hardtoremember       0.00      0.00      0.00         1\n",
      "         herman_gill       0.14      0.25      0.18         4\n",
      "      iamadogforreal       0.00      0.00      0.00         1\n",
      "        josiahpapaya       0.00      0.00      0.00         2\n",
      "           kibbleh21       0.00      0.00      0.00         4\n",
      "              kuvter       0.19      0.60      0.29         5\n",
      "           laterdude       0.00      0.00      0.00         2\n",
      "        lumpy_potato       0.00      0.00      0.00         3\n",
      "              mauxly       0.00      0.00      0.00         3\n",
      "            megatom0       0.00      0.00      0.00         2\n",
      "            misnamed       0.00      0.00      0.00         4\n",
      "         mynameipaul       0.00      0.00      0.00         3\n",
      "              myztry       0.00      0.00      0.00         4\n",
      "           nicholsml       0.00      0.00      0.00         3\n",
      "          nuclearwar       0.00      0.00      0.00         1\n",
      "       parlor_tricks       0.00      0.00      0.00         3\n",
      "          pixis-4950       0.04      1.00      0.07         1\n",
      "    probablydyslexic       0.00      0.00      0.00         2\n",
      "             rand486       0.00      0.00      0.00         4\n",
      "           redweasel       0.00      0.00      0.00         3\n",
      "        rivalarrival       0.00      0.00      0.00         1\n",
      "           robotevil       0.00      0.00      0.00         1\n",
      "        rogersmith25       0.00      0.00      0.00         1\n",
      "           shevagleb       0.00      0.00      0.00         2\n",
      "             skpkzk2       0.00      0.00      0.00         2\n",
      "            smacksaw       0.00      0.00      0.00         3\n",
      "              tubcat       0.00      0.00      0.00         3\n",
      "               twwwy       0.00      0.00      0.00         3\n",
      "      veni_vidi_vale       0.00      0.00      0.00         2\n",
      "            viramola       0.00      0.00      0.00         1\n",
      "          well_golly       0.00      0.00      0.00         1\n",
      " what_a_cat_astrophe       0.00      0.00      0.00         2\n",
      "        wonderfuldog       0.00      0.00      0.00         2\n",
      "         ychromosome       0.00      0.00      0.00         1\n",
      "\n",
      "           micro avg       0.11      0.11      0.11       292\n",
      "           macro avg       0.04      0.07      0.04       292\n",
      "        weighted avg       0.05      0.11      0.06       292\n",
      "\n",
      "Authors in test set:\n",
      "Label 1: A_Polite_Noise\n",
      "Label 2: Anomander\n",
      "Label 3: BZenMojo\n",
      "Label 4: Batty-Koda\n",
      "Label 5: Blenderhead36\n",
      "Label 6: BluepillProfessor\n",
      "Label 7: BuildMyPaperHeart\n",
      "Label 8: CaspianX2\n",
      "Label 9: Cebus_capucinus\n",
      "Label 10: Chaipod\n",
      "Label 11: CocoSavege\n",
      "Label 12: DashingLeech\n",
      "Label 13: Death_Star_\n",
      "Label 14: DejaBoo\n",
      "Label 15: DesCo83\n",
      "Label 16: Dickcheese_McDoogles\n",
      "Label 17: EnzoYug\n",
      "Label 18: FalconOne\n",
      "Label 19: Fearlessleader85\n",
      "Label 21: FrankManic\n",
      "Label 22: HeathenCyclist\n",
      "Label 23: Hiddencamper\n",
      "Label 24: InternetFree\n",
      "Label 25: IrrelevantTLDR\n",
      "Label 26: Kalium\n",
      "Label 27: KoNP\n",
      "Label 28: LarrySDonald\n",
      "Label 29: Lord_NiteShade\n",
      "Label 31: MathPolice\n",
      "Label 32: MegalomaniacHack\n",
      "Label 33: Monso\n",
      "Label 34: Mzsickness\n",
      "Label 35: Nightmathzombie\n",
      "Label 36: NukeThePope\n",
      "Label 37: Osiris32\n",
      "Label 38: Philo_T_Farnsworth\n",
      "Label 39: RamsesThePigeon\n",
      "Label 40: Rancid_Bear_Meat\n",
      "Label 41: RedditAntiHero\n",
      "Label 42: SearingPhoenix\n",
      "Label 43: ShakoraDrake\n",
      "Label 44: Shaper_pmp\n",
      "Label 45: ShortWoman\n",
      "Label 46: Stingray88\n",
      "Label 47: TheRealAlfredAdler\n",
      "Label 48: TheRufmeisterGeneral\n",
      "Label 49: ThreeTimesUp\n",
      "Label 50: ThrustVectoring\n",
      "Label 51: TooManyInLitter\n",
      "Label 52: Viperbunny\n",
      "Label 53: WalkerEU\n",
      "Label 54: ZeroNihilist\n",
      "Label 55: Zircon88\n",
      "Label 56: adelie42\n",
      "Label 57: adrianmonk\n",
      "Label 58: afcagroo\n",
      "Label 59: ajsdklf9df\n",
      "Label 60: alexisaacs\n",
      "Label 61: andytuba\n",
      "Label 62: angelworks\n",
      "Label 63: ashole11\n",
      "Label 64: ass_munch_reborn\n",
      "Label 65: avapoet\n",
      "Label 66: bmcclure937\n",
      "Label 67: branewalker\n",
      "Label 68: cabbagery\n",
      "Label 69: chadillac83\n",
      "Label 71: codayus\n",
      "Label 72: devedander\n",
      "Label 73: dinosaur_train\n",
      "Label 74: doesurmindglow\n",
      "Label 75: droidblaster\n",
      "Label 76: dsprox\n",
      "Label 77: dumboy\n",
      "Label 78: emeraldrumm\n",
      "Label 79: eroverton\n",
      "Label 80: feureau\n",
      "Label 81: freedomweasel\n",
      "Label 82: gingerkid1234\n",
      "Label 83: greenRiverThriller\n",
      "Label 84: hardtoremember\n",
      "Label 85: herman_gill\n",
      "Label 86: iamadogforreal\n",
      "Label 87: josiahpapaya\n",
      "Label 88: kibbleh21\n",
      "Label 89: kuvter\n",
      "Label 90: laterdude\n",
      "Label 91: lumpy_potato\n",
      "Label 92: mauxly\n",
      "Label 93: megatom0\n",
      "Label 94: misnamed\n",
      "Label 96: mynameipaul\n",
      "Label 97: myztry\n",
      "Label 99: nicholsml\n",
      "Label 101: nuclearwar\n",
      "Label 102: parlor_tricks\n",
      "Label 103: pixis-4950\n",
      "Label 104: probablydyslexic\n",
      "Label 105: rand486\n",
      "Label 106: redweasel\n",
      "Label 107: rivalarrival\n",
      "Label 108: robotevil\n",
      "Label 109: rogersmith25\n",
      "Label 110: shevagleb\n",
      "Label 111: skpkzk2\n",
      "Label 112: smacksaw\n",
      "Label 114: tubcat\n",
      "Label 115: twwwy\n",
      "Label 117: veni_vidi_vale\n",
      "Label 118: viramola\n",
      "Label 119: well_golly\n",
      "Label 120: what_a_cat_astrophe\n",
      "Label 121: wonderfuldog\n",
      "Label 122: ychromosome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\bertopic-test\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Replace the BERT tokenizer with RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "val_encodings = tokenize_function(val_texts.tolist())\n",
    "test_encodings = tokenize_function(test_texts.tolist())\n",
    "\n",
    "# Dataset class\n",
    "class RedditDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RedditDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = RedditDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = RedditDataset(test_encodings, test_labels.tolist())\n",
    "\n",
    "# Replace the BERT model with RoBERTa\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=len(le.classes_))\n",
    "\n",
    "# Adjust training arguments for the larger model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,  # You might need fewer epochs with a more powerful model\n",
    "    per_device_train_batch_size=8,  # Reduced batch size due to larger model\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2,  # This effectively doubles the batch size\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# After making predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Get the unique classes in our test set\n",
    "unique_classes = np.unique(test_labels)\n",
    "\n",
    "# Create a mapping from the original label encoder to the classes in our test set\n",
    "label_map = {i: le.classes_[i] for i in unique_classes}\n",
    "\n",
    "# Generate the classification report\n",
    "print(accuracy_score(test_labels, preds))\n",
    "print(classification_report(test_labels, preds, \n",
    "                            target_names=[label_map[i] for i in sorted(label_map.keys())],\n",
    "                            labels=sorted(label_map.keys())))\n",
    "\n",
    "# If you want to see which authors are in the test set\n",
    "print(\"Authors in test set:\")\n",
    "for i, author in label_map.items():\n",
    "    print(f\"Label {i}: {author}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986]\n",
      "Iteration: \tTime taken: 37867.436885834ms\n",
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986]\n",
      "Iteration: \tTime taken: 0.000000000ms\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'lookup' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m start3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     40\u001b[0m lookup \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m([super_ultra_fib(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m)])\n\u001b[0;32m     42\u001b[0m end3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end3\u001b[38;5;241m-\u001b[39mstart3)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.09f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 41\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m start3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     40\u001b[0m lookup \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[43msuper_ultra_fib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m)])\n\u001b[0;32m     42\u001b[0m end3 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end3\u001b[38;5;241m-\u001b[39mstart3)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.09f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36msuper_ultra_fib\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lookup[n]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mlookup\u001b[49m\u001b[38;5;241m.\u001b[39mappend(super_ultra_fib(n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m lookup[n\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lookup[n]\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'lookup' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# Fib\n",
    "def fibonacci_of(n):\n",
    "    if n in {0, 1}:  # Base case\n",
    "        return n\n",
    "    return fibonacci_of(n - 1) + fibonacci_of(n - 2)  # Recursive case\n",
    "\n",
    "def ultra_fib(n, lookup):\n",
    "    if n<=2:  # Base case\n",
    "        lookup.append(0)\n",
    "        lookup.append(1)\n",
    "        lookup.append(lookup[0] + lookup[1])\n",
    "        return lookup[n]\n",
    "    if n>2:\n",
    "        lookup.append(ultra_fib(n-1,lookup) + lookup[n-2])\n",
    "        return lookup[n]\n",
    "    \n",
    "def super_ultra_fib(n):\n",
    "    if n<=2:  # Base case\n",
    "        lookup = []\n",
    "        lookup.append(0)\n",
    "        lookup.append(1)\n",
    "        lookup.append(lookup[0] + lookup[1])\n",
    "        return lookup[n]\n",
    "    if n>2:\n",
    "        lookup.append(super_ultra_fib(n-1) + lookup[n-2])\n",
    "        return lookup[n]\n",
    "    \n",
    "start = time.time()\n",
    "print([fibonacci_of(n) for n in range(40)])\n",
    "end = time.time()\n",
    "print(f\"Iteration: \\tTime taken: {(end-start)*10**3:.09f}ms\")\n",
    "\n",
    "start2 = time.time()\n",
    "print([ultra_fib(n, []) for n in range(40)])\n",
    "end2 = time.time()\n",
    "print(f\"Iteration: \\tTime taken: {(end2-start2)*10**3:.09f}ms\")\n",
    "\n",
    "start3 = time.time()\n",
    "lookup = []\n",
    "print([super_ultra_fib(n) for n in range(40)])\n",
    "end3 = time.time()\n",
    "print(f\"Iteration: \\tTime taken: {(end3-start3)*10**3:.09f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
