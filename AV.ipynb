{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage per batch: 1953.46 MB\n",
      "Estimated total memory usage for one epoch: 609477.98 MB\n",
      "Fold 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [38:58<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 1414.74 MB\n",
      "Epoch 1/2, Train Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0283, Accuracy: 0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 157/157 [20:59<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -798.73 MB\n",
      "Epoch 2/2, Train Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0135, Accuracy: 0.9974\n",
      "Fold 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [39:49<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -1099.51 MB\n",
      "Epoch 1/2, Train Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0176, Accuracy: 0.9946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 157/157 [21:40<00:00,  8.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -16.70 MB\n",
      "Epoch 2/2, Train Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0122, Accuracy: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mcant\\AppData\\Local\\Temp\\ipykernel_38180\\1776905214.py:276: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('best_authorship_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts are by the same author: True\n",
      "Similarity score: 0.5149\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "#os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class RedditAuthorshipDataset(Dataset):\n",
    "    def __init__(self, texts, authors, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.authors = authors\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1, author1 = self.texts[idx], self.authors[idx]\n",
    "        # Randomly select another sample\n",
    "        other_idx = np.random.randint(len(self.texts))\n",
    "        text2, author2 = self.texts[other_idx], self.authors[other_idx]\n",
    "        \n",
    "        label = 1 if author1 == author2 else 0\n",
    "\n",
    "        encoding1 = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer.encode_plus(\n",
    "            text2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids1': encoding1['input_ids'].flatten(),\n",
    "            'attention_mask1': encoding1['attention_mask'].flatten(),\n",
    "            'input_ids2': encoding2['input_ids'].flatten(),\n",
    "            'attention_mask2': encoding2['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class ContrastiveAuthorshipModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='roberta-base'):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.roberta(input_ids1, attention_mask=attention_mask1)\n",
    "        output2 = self.roberta(input_ids2, attention_mask=attention_mask2)\n",
    "        \n",
    "        embedding1 = self.projection(self.dropout(output1.last_hidden_state[:, 0, :]))\n",
    "        embedding2 = self.projection(self.dropout(output2.last_hidden_state[:, 0, :]))\n",
    "        \n",
    "        return embedding1, embedding2\n",
    "\n",
    "\n",
    "def contrastive_loss(embedding1, embedding2, label, temperature=1):\n",
    "    cosine_similarity = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    similarity_scaled = cosine_similarity / temperature\n",
    "    loss = torch.mean((1 - label) * torch.pow(torch.clamp(similarity_scaled, min=0.0), 2) +\n",
    "                      label * torch.pow(1 - similarity_scaled, 2))\n",
    "    return loss\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, device, epochs=5, patience=3):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_memory = get_memory_usage()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = contrastive_loss(embedding1, embedding2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_memory = get_memory_usage()\n",
    "        print(f\"Memory Usage: {end_memory - start_memory:.2f} MB\")\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation step\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_authorship_model.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = contrastive_loss(embedding1, embedding2, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            similarity = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "            predictions = (similarity > 0.5).long()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def predict_authorship(model, tokenizer, text1, text2, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    encoding1 = tokenizer.encode_plus(\n",
    "        text1,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoding2 = tokenizer.encode_plus(\n",
    "        text2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids1 = encoding1['input_ids'].to(device)\n",
    "    attention_mask1 = encoding1['attention_mask'].to(device)\n",
    "    input_ids2 = encoding2['input_ids'].to(device)\n",
    "    attention_mask2 = encoding2['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        similarity = nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "    \n",
    "    same_author = similarity > threshold\n",
    "    return same_author, similarity\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "\n",
    "def estimate_memory_usage(model, batch_size, seq_length, dtype=torch.float32):\n",
    "    def numel(model):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Model parameters\n",
    "    model_params_memory = numel(model) * dtype.itemsize\n",
    "\n",
    "    # Estimate memory for one forward + backward pass\n",
    "    input_size = batch_size * seq_length\n",
    "    activations_memory = input_size * model.roberta.config.hidden_size * dtype.itemsize * 2  # *2 for forward and backward\n",
    "    gradients_memory = model_params_memory\n",
    "    \n",
    "    # Optimizer memory (assuming Adam)\n",
    "    optimizer_memory = model_params_memory * 2  # Adam keeps two additional values per parameter\n",
    "\n",
    "    # Estimate memory for embeddings\n",
    "    embedding_memory = batch_size * 2 * seq_length * model.roberta.config.hidden_size * dtype.itemsize\n",
    "\n",
    "    # Estimate memory for attention masks\n",
    "    attention_mask_memory = batch_size * 2 * seq_length * torch.bool.itemsize\n",
    "\n",
    "    # Total estimated memory\n",
    "    total_memory = (model_params_memory + activations_memory + gradients_memory + \n",
    "                    optimizer_memory + embedding_memory + attention_mask_memory)\n",
    "\n",
    "    # Convert to MB\n",
    "    total_memory_mb = total_memory / (1024 * 1024)\n",
    "\n",
    "    return total_memory_mb\n",
    "\n",
    "### Main\n",
    "# Load Reddit dataset\n",
    "data = load_dataset(\"reddit\", split=\"train[:10000]\", trust_remote_code=True)\n",
    "texts = data['content']\n",
    "authors = data['author']\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "seq_length = 128  # Max sequence length\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "dataset = RedditAuthorshipDataset(texts, authors, tokenizer)\n",
    "\n",
    "model = ContrastiveAuthorshipModel().to(device)\n",
    "#model.load_state_dict(torch.load('best_authorship_model.pth'))\n",
    "\n",
    "# Estimate memory usage\n",
    "estimated_memory = estimate_memory_usage(model, batch_size, seq_length)\n",
    "print(f\"Estimated memory usage per batch: {estimated_memory:.2f} MB\")\n",
    "\n",
    "# Estimate for entire dataset\n",
    "dataset_size = 10000  # Adjust this to your actual dataset size\n",
    "num_batches = dataset_size // batch_size\n",
    "total_estimated_memory = estimated_memory * num_batches\n",
    "print(f\"Estimated total memory usage for one epoch: {total_estimated_memory:.2f} MB\")\n",
    "\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset), 1):\n",
    "    print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=32, sampler=val_subsampler)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    model = train(model, train_loader, val_loader, optimizer, device, epochs=2, patience=1)\n",
    "\n",
    "# Load the best model for prediction\n",
    "best_model = ContrastiveAuthorshipModel().to(device)\n",
    "best_model.load_state_dict(torch.load('best_authorship_model.pth'))\n",
    "\n",
    "# Example prediction\n",
    "text1 = \"\"\"There were no Dark Ages! They didn't happen! Byzantium was happily being Byzantium. The Muslims were doing fucking amazing things! Al-Andalus was a beacon of cultural integration, art, science, and philosophy! Ibn Khaldun was inventing modern history! \n",
    "        The 'Dark Ages' where when a bunch of dirt sucking savages from east-bumfuck lost contact with the First World, which is to say the Mediterranean. It was 'Dark' because no one who mattered gave two shits what was happening in Germania because Germania was utterly irrelevant to the world economy, sciences, history, and politics. Europe went through the 'Dark Ages' because Europe was not important. It was a worthless, cold, savage back water full of dirty hairy people who wore pants. \n",
    "        Right up until 700ish, when the Scandanavians went a viking and started to spread their culture across Northern Europe, setting up trade across the continent, forcing other NE cultures to centralize and become more efficient to resist the north men. \n",
    "        Seriously, though, the Muslims were rocking out with their Qu'ran out after about 600, and they did more for art, science, philosophy, and poetry than the Romans had done since 100ad. The period of Muslim ascendancy flowed smoothly out of the fall of Western Rome and then snugged seamlessly into the Renaissance. \n",
    "        And then the Norse were doing all sorts of wacky stuff with democracy and law from the mid millenium. Really, if there was a 'Dark Age' it was only from about 450, when the Romans abandoned Italy, to abou 600, when the Muslims really started kicking ass and taking names. \n",
    "        The only thing that was really 'lost' with the fall of Western Rome was the extremely powerful and centralized Roman state. All the cool technology they had persisted in other places (Specifically, everywhere except Europe), but without the massive centralization that let the Romans make use of it on such a large scale.\"\"\"\n",
    "text2 = \"\"\"I would associate the decline of the church largely to the loss of power of the Roman Empire in germania and western Europe, which was due to a large number of complicated factors, including over extension of the Empire's resources, migration of 'barbarian' peoples into the empire, the conflict between Pagan Roman religion and Christianity (Fun fact, the Visigoths that sacked Rome were Arian Christians, followers of a creed that had been declared heretical at the council of Nicea), and many, many other things. The Roman Empire was extremely important to pre-medieval Europe, introducing all kinds of culture and technology. When the financial and military support of the Empire withdrew much of that culture and technology went with it. \n",
    "        Also, I would like to note that up until... hmm, probably the 1500s or 1600s many, many powerful political figures were members were both Clergy and princes. Many Bishops and other church figures held land, raised armies, went to war, and participated in the councils of kings. They fought with secular lords and also with each other. \n",
    "        I would not say that the Church caused any decline in Europe, on the grounds that in many way there is no Europe without the Church and their is no Church without Europe. Catholicism was the culture of Europe from around 500 to around 1700. The Church was as important and basic a component of culture at that time as the Internet is now. Priests were often the only people with a semblance of education, the only people able to write and receive letters. While some theologians certainly advocated a radical and oppressive form of Christianity, others provided council to their leaders that served to limit the gross abuses of Feudalism. \n",
    "        In the end it's far too complicated to say that the Christian Church was a good thing or a bad thing. It spurred Europe to destructive wars with the Muslims to the south and the pagan Slavs in the east. It provided the foundations for rational inquiry on which Science was founded. It founded and promulgated the Inquisition, which was both a machine of torture and oppression and an instrument of social and political justice. The church preserved knowledge from the time of Rome and suppressed new knowledge. The church contributed and obstructed philosophy. \n",
    "        If Catholicism hadn't become the dominant religion in Europe I don't know that things would have changed very much. Certainly a Europe that followed the Mithras cult or kept to Roman or Germanic Paganism would be different, but I don't think humanity would necessarily have made more social or technical progress. The Romans could be as brutal and sadistic as any Inquistor, and the Vikings were notorious for being savage in battle. The Muslims put whole cities to the sword, and the Mongols carved a swath across the entire world. If Roman Catholicism hadn't risen to become the dominant cultural framework of Europe then it seems likely to me that one of those four groups, the Muslims, the Norsemen, the Romans, or the Mongols, would have shaped the face of Europe. Each culture had its great triumphs and terrible deeds.\"\"\"\n",
    "same_author, similarity = predict_authorship(best_model, tokenizer, text1, text2, device)\n",
    "print(f\"Texts are by the same author: {same_author}\")\n",
    "print(f\"Similarity score: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mcant\\AppData\\Local\\Temp\\ipykernel_38180\\3009939459.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_authorship_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "t:  1\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "t:  2\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "t:  3\n",
      "e:  1\n",
      "t:  4\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "t:  5\n",
      "t:  6\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "t:  7\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "e:  37\n",
      "e:  38\n",
      "e:  39\n",
      "e:  40\n",
      "e:  41\n",
      "e:  42\n",
      "e:  43\n",
      "e:  44\n",
      "e:  45\n",
      "e:  46\n",
      "e:  47\n",
      "e:  48\n",
      "t:  8\n",
      "e:  1\n",
      "t:  9\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "t:  10\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "e:  37\n",
      "e:  38\n",
      "e:  39\n",
      "e:  40\n",
      "t:  11\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "e:  37\n",
      "e:  38\n",
      "e:  39\n",
      "e:  40\n",
      "e:  41\n",
      "e:  42\n",
      "e:  43\n",
      "e:  44\n",
      "e:  45\n",
      "e:  46\n",
      "t:  12\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "t:  13\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  14\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "t:  15\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "t:  16\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "t:  17\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "t:  18\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "t:  19\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "t:  20\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "t:  21\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "t:  22\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "t:  23\n",
      "e:  1\n",
      "e:  2\n",
      "t:  24\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "e:  37\n",
      "e:  38\n",
      "e:  39\n",
      "e:  40\n",
      "e:  41\n",
      "e:  42\n",
      "e:  43\n",
      "e:  44\n",
      "t:  25\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "t:  26\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "t:  27\n",
      "e:  1\n",
      "t:  28\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "t:  29\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  30\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "e:  37\n",
      "e:  38\n",
      "e:  39\n",
      "e:  40\n",
      "e:  41\n",
      "e:  42\n",
      "e:  43\n",
      "e:  44\n",
      "e:  45\n",
      "e:  46\n",
      "e:  47\n",
      "e:  48\n",
      "e:  49\n",
      "e:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ContrastiveAuthorshipModel' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest F1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(f1_scores)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 104\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    102\u001b[0m f1_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 104\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_similarities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 71\u001b[0m, in \u001b[0;36mvalidate_similarities\u001b[1;34m(model, pairs)\u001b[0m\n\u001b[0;32m     69\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text1, text2 \u001b[38;5;129;01min\u001b[39;00m tqdm(pairs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing similarities\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 71\u001b[0m     emb1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     emb2 \u001b[38;5;241m=\u001b[39m get_embedding(model, text2)\n\u001b[0;32m     73\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(emb1, emb2)\n",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(model, text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(model, text):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(text)\n",
      "File \u001b[1;32mc:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ContrastiveAuthorshipModel' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def get_embedding(model, text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def model_similarity(model, a, b):\n",
    "    return model.similarity(a, b)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "#def create_pairs(texts, authors, n_pairs):\n",
    "#    pairs = []\n",
    "#    labels = []\n",
    "#    for _ in range(n_pairs):\n",
    "#        idx1, idx2 = np.random.choice(len(texts), 2, replace=False)\n",
    "#        pairs.append((texts[idx1], texts[idx2]))\n",
    "#        labels.append(int(authors[idx1] == authors[idx2]))\n",
    "#    return pairs, labels\n",
    "\n",
    "def create_pairs(texts, authors, n_pairs):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    unique_authors = list(set(authors))\n",
    "    e = 0\n",
    "    t = 0\n",
    "    \n",
    "    # Create positive pairs (same author)\n",
    "    n_positive = n_pairs // 2\n",
    "    print(n_positive)\n",
    "    while t < n_positive and e < 50:\n",
    "        author = np.random.choice(unique_authors)\n",
    "        author_texts = [text for text, a in zip(texts, authors) if a == author]\n",
    "        if len(author_texts) < 2:\n",
    "            e+=1\n",
    "            print(\"e: \", e)\n",
    "            continue\n",
    "        text1, text2 = np.random.choice(author_texts, 2, replace=False)\n",
    "        pairs.append((text1, text2))\n",
    "        labels.append(1)\n",
    "        t+=1\n",
    "        print(\"t: \", t)\n",
    "        e=0\n",
    "    \n",
    "    # Create negative pairs (different authors)\n",
    "    n_negative = n_pairs - len(pairs)\n",
    "    for _ in range(n_negative):\n",
    "        author1, author2 = np.random.choice(unique_authors, 2, replace=False)\n",
    "        text1 = np.random.choice([text for text, a in zip(texts, authors) if a == author1])\n",
    "        text2 = np.random.choice([text for text, a in zip(texts, authors) if a == author2])\n",
    "        pairs.append((text1, text2))\n",
    "        labels.append(0)\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "def predict_authorship(model, tokenizer, text1, text2, device):\n",
    "    model.eval()\n",
    "    encoding1 = tokenizer.encode_plus(\n",
    "        text1,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoding2 = tokenizer.encode_plus(\n",
    "        text2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids1 = encoding1['input_ids'].to(device)\n",
    "    attention_mask1 = encoding1['attention_mask'].to(device)\n",
    "    input_ids2 = encoding2['input_ids'].to(device)\n",
    "    attention_mask2 = encoding2['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        similarity = nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def validate_similarities(model, tokenizer, pairs, device):\n",
    "    similarities = []\n",
    "    for text1, text2 in tqdm(pairs, desc=\"Computing similarities\"):\n",
    "        similarity = predict_authorship(model, tokenizer, text1, text2, device)\n",
    "        #similarity = model_similarity(model, emb1, emb2)\n",
    "        similarities.append(similarity)\n",
    "    #softsims = softmax(similarities)\n",
    "    return similarities\n",
    "\n",
    "def validate_threshold(labels, threshold, similarities):\n",
    "    predictions = [int(sim > threshold) for sim in similarities]\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return accuracy, f1\n",
    "\n",
    "def main():\n",
    "    # Load Reddit dataset\n",
    "    data = load_dataset(\"reddit\", split=\"train[10000:20000]\", trust_remote_code=True)\n",
    "    texts = data['content']\n",
    "    authors = data['author']\n",
    "\n",
    "    import gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    #load tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    #Load embedding model\n",
    "    #model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    model = ContrastiveAuthorshipModel().to(device)\n",
    "    model.load_state_dict(torch.load('best_authorship_model.pth'))\n",
    "\n",
    "    # Create pairs for validation\n",
    "    pairs, labels = create_pairs(texts, authors, n_pairs=100)\n",
    "\n",
    "    # Validate thresholds\n",
    "    thresholds = np.arange(0.1, 0.8, 0.1)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    similarities = validate_similarities(model, tokenizer, pairs, device)\n",
    "    for threshold in thresholds:\n",
    "        print(f\"Validating threshold: {threshold}\")\n",
    "        accuracy, f1 = validate_threshold(labels, threshold, similarities)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")\n",
    "\n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, accuracies, label='Accuracy', marker='o')\n",
    "    plt.plot(thresholds, f1_scores, label='F1 Score', marker='s')\n",
    "    plt.xlabel('Similarity Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Accuracy and F1 Score vs Similarity Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('validation_results.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Find best threshold\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {max(f1_scores)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
