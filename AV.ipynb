{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage per batch: 1953.46 MB\n",
      "Estimated total memory usage for one epoch: 609477.98 MB\n",
      "Fold 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [38:58<00:00, 14.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 1414.74 MB\n",
      "Epoch 1/2, Train Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0283, Accuracy: 0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 157/157 [20:59<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -798.73 MB\n",
      "Epoch 2/2, Train Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0135, Accuracy: 0.9974\n",
      "Fold 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 157/157 [39:49<00:00, 15.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -1099.51 MB\n",
      "Epoch 1/2, Train Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:00<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0176, Accuracy: 0.9946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 157/157 [21:40<00:00,  8.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: -16.70 MB\n",
      "Epoch 2/2, Train Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [02:03<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0122, Accuracy: 0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mcant\\AppData\\Local\\Temp\\ipykernel_38180\\1776905214.py:276: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('best_authorship_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts are by the same author: True\n",
      "Similarity score: 0.5149\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "#os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "class RedditAuthorshipDataset(Dataset):\n",
    "    def __init__(self, texts, authors, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.authors = authors\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1, author1 = self.texts[idx], self.authors[idx]\n",
    "        # Randomly select another sample\n",
    "        other_idx = np.random.randint(len(self.texts))\n",
    "        text2, author2 = self.texts[other_idx], self.authors[other_idx]\n",
    "        \n",
    "        label = 1 if author1 == author2 else 0\n",
    "\n",
    "        encoding1 = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer.encode_plus(\n",
    "            text2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids1': encoding1['input_ids'].flatten(),\n",
    "            'attention_mask1': encoding1['attention_mask'].flatten(),\n",
    "            'input_ids2': encoding2['input_ids'].flatten(),\n",
    "            'attention_mask2': encoding2['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class ContrastiveAuthorshipModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='roberta-base'):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.roberta(input_ids1, attention_mask=attention_mask1)\n",
    "        output2 = self.roberta(input_ids2, attention_mask=attention_mask2)\n",
    "        \n",
    "        embedding1 = self.projection(self.dropout(output1.last_hidden_state[:, 0, :]))\n",
    "        embedding2 = self.projection(self.dropout(output2.last_hidden_state[:, 0, :]))\n",
    "        \n",
    "        return embedding1, embedding2\n",
    "\n",
    "\n",
    "def contrastive_loss(embedding1, embedding2, label, temperature=1):\n",
    "    cosine_similarity = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "    similarity_scaled = cosine_similarity / temperature\n",
    "    loss = torch.mean((1 - label) * torch.pow(torch.clamp(similarity_scaled, min=0.0), 2) +\n",
    "                      label * torch.pow(1 - similarity_scaled, 2))\n",
    "    return loss\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, device, epochs=5, patience=3):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_memory = get_memory_usage()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = contrastive_loss(embedding1, embedding2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        end_memory = get_memory_usage()\n",
    "        print(f\"Memory Usage: {end_memory - start_memory:.2f} MB\")\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation step\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, device)\n",
    "        print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_authorship_model.pth')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids1 = batch['input_ids1'].to(device)\n",
    "            attention_mask1 = batch['attention_mask1'].to(device)\n",
    "            input_ids2 = batch['input_ids2'].to(device)\n",
    "            attention_mask2 = batch['attention_mask2'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "            loss = contrastive_loss(embedding1, embedding2, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            similarity = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "            predictions = (similarity > 0.5).long()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def predict_authorship(model, tokenizer, text1, text2, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    encoding1 = tokenizer.encode_plus(\n",
    "        text1,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoding2 = tokenizer.encode_plus(\n",
    "        text2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids1 = encoding1['input_ids'].to(device)\n",
    "    attention_mask1 = encoding1['attention_mask'].to(device)\n",
    "    input_ids2 = encoding2['input_ids'].to(device)\n",
    "    attention_mask2 = encoding2['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        similarity = nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "    \n",
    "    same_author = similarity > threshold\n",
    "    return same_author, similarity\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # in MB\n",
    "\n",
    "def estimate_memory_usage(model, batch_size, seq_length, dtype=torch.float32):\n",
    "    def numel(model):\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    # Model parameters\n",
    "    model_params_memory = numel(model) * dtype.itemsize\n",
    "\n",
    "    # Estimate memory for one forward + backward pass\n",
    "    input_size = batch_size * seq_length\n",
    "    activations_memory = input_size * model.roberta.config.hidden_size * dtype.itemsize * 2  # *2 for forward and backward\n",
    "    gradients_memory = model_params_memory\n",
    "    \n",
    "    # Optimizer memory (assuming Adam)\n",
    "    optimizer_memory = model_params_memory * 2  # Adam keeps two additional values per parameter\n",
    "\n",
    "    # Estimate memory for embeddings\n",
    "    embedding_memory = batch_size * 2 * seq_length * model.roberta.config.hidden_size * dtype.itemsize\n",
    "\n",
    "    # Estimate memory for attention masks\n",
    "    attention_mask_memory = batch_size * 2 * seq_length * torch.bool.itemsize\n",
    "\n",
    "    # Total estimated memory\n",
    "    total_memory = (model_params_memory + activations_memory + gradients_memory + \n",
    "                    optimizer_memory + embedding_memory + attention_mask_memory)\n",
    "\n",
    "    # Convert to MB\n",
    "    total_memory_mb = total_memory / (1024 * 1024)\n",
    "\n",
    "    return total_memory_mb\n",
    "\n",
    "### Main\n",
    "# Load Reddit dataset\n",
    "data = load_dataset(\"reddit\", split=\"train[:10000]\", trust_remote_code=True)\n",
    "texts = data['content']\n",
    "authors = data['author']\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "seq_length = 128  # Max sequence length\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "dataset = RedditAuthorshipDataset(texts, authors, tokenizer)\n",
    "\n",
    "model = ContrastiveAuthorshipModel().to(device)\n",
    "#model.load_state_dict(torch.load('best_authorship_model.pth'))\n",
    "\n",
    "# Estimate memory usage\n",
    "estimated_memory = estimate_memory_usage(model, batch_size, seq_length)\n",
    "print(f\"Estimated memory usage per batch: {estimated_memory:.2f} MB\")\n",
    "\n",
    "# Estimate for entire dataset\n",
    "dataset_size = 10000  # Adjust this to your actual dataset size\n",
    "num_batches = dataset_size // batch_size\n",
    "total_estimated_memory = estimated_memory * num_batches\n",
    "print(f\"Estimated total memory usage for one epoch: {total_estimated_memory:.2f} MB\")\n",
    "\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset), 1):\n",
    "    print(f\"Fold {fold}/{n_splits}\")\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=32, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=32, sampler=val_subsampler)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    model = train(model, train_loader, val_loader, optimizer, device, epochs=2, patience=1)\n",
    "\n",
    "# Load the best model for prediction\n",
    "best_model = ContrastiveAuthorshipModel().to(device)\n",
    "best_model.load_state_dict(torch.load('best_authorship_model.pth'))\n",
    "\n",
    "# Example prediction\n",
    "text1 = \"\"\"There were no Dark Ages! They didn't happen! Byzantium was happily being Byzantium. The Muslims were doing fucking amazing things! Al-Andalus was a beacon of cultural integration, art, science, and philosophy! Ibn Khaldun was inventing modern history! \n",
    "        The 'Dark Ages' where when a bunch of dirt sucking savages from east-bumfuck lost contact with the First World, which is to say the Mediterranean. It was 'Dark' because no one who mattered gave two shits what was happening in Germania because Germania was utterly irrelevant to the world economy, sciences, history, and politics. Europe went through the 'Dark Ages' because Europe was not important. It was a worthless, cold, savage back water full of dirty hairy people who wore pants. \n",
    "        Right up until 700ish, when the Scandanavians went a viking and started to spread their culture across Northern Europe, setting up trade across the continent, forcing other NE cultures to centralize and become more efficient to resist the north men. \n",
    "        Seriously, though, the Muslims were rocking out with their Qu'ran out after about 600, and they did more for art, science, philosophy, and poetry than the Romans had done since 100ad. The period of Muslim ascendancy flowed smoothly out of the fall of Western Rome and then snugged seamlessly into the Renaissance. \n",
    "        And then the Norse were doing all sorts of wacky stuff with democracy and law from the mid millenium. Really, if there was a 'Dark Age' it was only from about 450, when the Romans abandoned Italy, to abou 600, when the Muslims really started kicking ass and taking names. \n",
    "        The only thing that was really 'lost' with the fall of Western Rome was the extremely powerful and centralized Roman state. All the cool technology they had persisted in other places (Specifically, everywhere except Europe), but without the massive centralization that let the Romans make use of it on such a large scale.\"\"\"\n",
    "text2 = \"\"\"I would associate the decline of the church largely to the loss of power of the Roman Empire in germania and western Europe, which was due to a large number of complicated factors, including over extension of the Empire's resources, migration of 'barbarian' peoples into the empire, the conflict between Pagan Roman religion and Christianity (Fun fact, the Visigoths that sacked Rome were Arian Christians, followers of a creed that had been declared heretical at the council of Nicea), and many, many other things. The Roman Empire was extremely important to pre-medieval Europe, introducing all kinds of culture and technology. When the financial and military support of the Empire withdrew much of that culture and technology went with it. \n",
    "        Also, I would like to note that up until... hmm, probably the 1500s or 1600s many, many powerful political figures were members were both Clergy and princes. Many Bishops and other church figures held land, raised armies, went to war, and participated in the councils of kings. They fought with secular lords and also with each other. \n",
    "        I would not say that the Church caused any decline in Europe, on the grounds that in many way there is no Europe without the Church and their is no Church without Europe. Catholicism was the culture of Europe from around 500 to around 1700. The Church was as important and basic a component of culture at that time as the Internet is now. Priests were often the only people with a semblance of education, the only people able to write and receive letters. While some theologians certainly advocated a radical and oppressive form of Christianity, others provided council to their leaders that served to limit the gross abuses of Feudalism. \n",
    "        In the end it's far too complicated to say that the Christian Church was a good thing or a bad thing. It spurred Europe to destructive wars with the Muslims to the south and the pagan Slavs in the east. It provided the foundations for rational inquiry on which Science was founded. It founded and promulgated the Inquisition, which was both a machine of torture and oppression and an instrument of social and political justice. The church preserved knowledge from the time of Rome and suppressed new knowledge. The church contributed and obstructed philosophy. \n",
    "        If Catholicism hadn't become the dominant religion in Europe I don't know that things would have changed very much. Certainly a Europe that followed the Mithras cult or kept to Roman or Germanic Paganism would be different, but I don't think humanity would necessarily have made more social or technical progress. The Romans could be as brutal and sadistic as any Inquistor, and the Vikings were notorious for being savage in battle. The Muslims put whole cities to the sword, and the Mongols carved a swath across the entire world. If Roman Catholicism hadn't risen to become the dominant cultural framework of Europe then it seems likely to me that one of those four groups, the Muslims, the Norsemen, the Romans, or the Mongols, would have shaped the face of Europe. Each culture had its great triumphs and terrible deeds.\"\"\"\n",
    "same_author, similarity = predict_authorship(best_model, tokenizer, text1, text2, device)\n",
    "print(f\"Texts are by the same author: {same_author}\")\n",
    "print(f\"Similarity score: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mcant\\AppData\\Local\\Temp\\ipykernel_32404\\605277445.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_authorship_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  1\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  2\n",
      "e:  1\n",
      "e:  2\n",
      "t:  3\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "t:  4\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "t:  5\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  6\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  7\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "t:  8\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "t:  9\n",
      "e:  1\n",
      "t:  10\n",
      "e:  1\n",
      "e:  2\n",
      "t:  11\n",
      "e:  1\n",
      "t:  12\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "t:  13\n",
      "e:  1\n",
      "e:  2\n",
      "t:  14\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  15\n",
      "e:  1\n",
      "t:  16\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  17\n",
      "e:  1\n",
      "t:  18\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "t:  19\n",
      "e:  1\n",
      "e:  2\n",
      "t:  20\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "t:  21\n",
      "e:  1\n",
      "t:  22\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "t:  23\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  24\n",
      "e:  1\n",
      "e:  2\n",
      "t:  25\n",
      "t:  26\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "e:  35\n",
      "e:  36\n",
      "e:  37\n",
      "e:  38\n",
      "e:  39\n",
      "e:  40\n",
      "t:  27\n",
      "e:  1\n",
      "t:  28\n",
      "e:  1\n",
      "e:  2\n",
      "t:  29\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "t:  30\n",
      "t:  31\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  32\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  33\n",
      "t:  34\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "t:  35\n",
      "t:  36\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "t:  37\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  38\n",
      "e:  1\n",
      "t:  39\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "t:  40\n",
      "e:  1\n",
      "e:  2\n",
      "t:  41\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "t:  42\n",
      "e:  1\n",
      "t:  43\n",
      "e:  1\n",
      "t:  44\n",
      "t:  45\n",
      "e:  1\n",
      "t:  46\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "t:  47\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "t:  48\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "t:  49\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  50\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  51\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "t:  52\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  53\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  54\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "t:  55\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "t:  56\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  57\n",
      "e:  1\n",
      "t:  58\n",
      "t:  59\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "t:  60\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "t:  61\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "t:  62\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "t:  63\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  64\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "t:  65\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  66\n",
      "e:  1\n",
      "e:  2\n",
      "t:  67\n",
      "e:  1\n",
      "t:  68\n",
      "e:  1\n",
      "e:  2\n",
      "t:  69\n",
      "t:  70\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "e:  23\n",
      "e:  24\n",
      "e:  25\n",
      "e:  26\n",
      "e:  27\n",
      "e:  28\n",
      "e:  29\n",
      "e:  30\n",
      "e:  31\n",
      "e:  32\n",
      "e:  33\n",
      "e:  34\n",
      "t:  71\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "t:  72\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "t:  73\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "t:  74\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "t:  75\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "t:  76\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "t:  77\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  78\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "t:  79\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  80\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "e:  21\n",
      "e:  22\n",
      "t:  81\n",
      "e:  1\n",
      "e:  2\n",
      "t:  82\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "t:  83\n",
      "e:  1\n",
      "e:  2\n",
      "t:  84\n",
      "e:  1\n",
      "t:  85\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  86\n",
      "t:  87\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "t:  88\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "t:  89\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "t:  90\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "t:  91\n",
      "t:  92\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "t:  93\n",
      "t:  94\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "t:  95\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "t:  96\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "t:  97\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "e:  9\n",
      "e:  10\n",
      "e:  11\n",
      "e:  12\n",
      "e:  13\n",
      "e:  14\n",
      "e:  15\n",
      "e:  16\n",
      "e:  17\n",
      "e:  18\n",
      "e:  19\n",
      "e:  20\n",
      "t:  98\n",
      "e:  1\n",
      "e:  2\n",
      "e:  3\n",
      "e:  4\n",
      "e:  5\n",
      "e:  6\n",
      "e:  7\n",
      "e:  8\n",
      "t:  99\n",
      "e:  1\n",
      "e:  2\n",
      "t:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities: 100%|██████████| 200/200 [00:05<00:00, 36.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating threshold: 0.01\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.03\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.049999999999999996\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.06999999999999999\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.08999999999999998\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.10999999999999997\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.12999999999999998\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.15\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.16999999999999998\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.18999999999999997\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.20999999999999996\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.22999999999999998\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.24999999999999997\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.26999999999999996\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.29\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.30999999999999994\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.32999999999999996\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.35\n",
      "Accuracy: 0.5, F1 Score: 0.6666666666666666\n",
      "Validating threshold: 0.36999999999999994\n",
      "Accuracy: 0.5, F1 Score: 0.6644295302013423\n",
      "Validating threshold: 0.38999999999999996\n",
      "Accuracy: 0.495, F1 Score: 0.6552901023890785\n",
      "Validating threshold: 0.4099999999999999\n",
      "Accuracy: 0.51, F1 Score: 0.6597222222222222\n",
      "Validating threshold: 0.42999999999999994\n",
      "Accuracy: 0.515, F1 Score: 0.6498194945848376\n",
      "Validating threshold: 0.44999999999999996\n",
      "Accuracy: 0.5, F1 Score: 0.6268656716417911\n",
      "Validating threshold: 0.4699999999999999\n",
      "Accuracy: 0.525, F1 Score: 0.6245059288537549\n",
      "Validating threshold: 0.48999999999999994\n",
      "Accuracy: 0.555, F1 Score: 0.6244725738396625\n",
      "Validating threshold: 0.5099999999999999\n",
      "Accuracy: 0.575, F1 Score: 0.6009389671361502\n",
      "Validating threshold: 0.5299999999999999\n",
      "Accuracy: 0.585, F1 Score: 0.5699481865284974\n",
      "Validating threshold: 0.5499999999999999\n",
      "Accuracy: 0.615, F1 Score: 0.5497076023391813\n",
      "Validating threshold: 0.57\n",
      "Accuracy: 0.605, F1 Score: 0.48366013071895425\n",
      "Validating threshold: 0.59\n",
      "Accuracy: 0.6, F1 Score: 0.4444444444444444\n",
      "Validating threshold: 0.6099999999999999\n",
      "Accuracy: 0.57, F1 Score: 0.36764705882352944\n",
      "Validating threshold: 0.6299999999999999\n",
      "Accuracy: 0.535, F1 Score: 0.256\n",
      "Validating threshold: 0.6499999999999999\n",
      "Accuracy: 0.54, F1 Score: 0.2459016393442623\n",
      "Validating threshold: 0.6699999999999999\n",
      "Accuracy: 0.54, F1 Score: 0.19298245614035087\n",
      "Validating threshold: 0.69\n",
      "Accuracy: 0.525, F1 Score: 0.12844036697247707\n",
      "Validating threshold: 0.7099999999999999\n",
      "Accuracy: 0.515, F1 Score: 0.0761904761904762\n",
      "Validating threshold: 0.7299999999999999\n",
      "Accuracy: 0.51, F1 Score: 0.057692307692307696\n",
      "Validating threshold: 0.7499999999999999\n",
      "Accuracy: 0.51, F1 Score: 0.057692307692307696\n",
      "Validating threshold: 0.7699999999999999\n",
      "Accuracy: 0.515, F1 Score: 0.05825242718446602\n",
      "Validating threshold: 0.7899999999999999\n",
      "Accuracy: 0.515, F1 Score: 0.05825242718446602\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.01\n",
      "Best F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import psutil\n",
    "\n",
    "class ContrastiveAuthorshipModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='roberta-base', freeze_base=False):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        if freeze_base:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
    "        output1 = self.roberta(input_ids1, attention_mask=attention_mask1)\n",
    "        output2 = self.roberta(input_ids2, attention_mask=attention_mask2)\n",
    "        \n",
    "        pooled_output1 = self.mean_pooling(output1, attention_mask1)\n",
    "        pooled_output2 = self.mean_pooling(output2, attention_mask2)\n",
    "        \n",
    "        embedding1 = self.projection(pooled_output1)\n",
    "        embedding2 = self.projection(pooled_output2)\n",
    "        \n",
    "        return embedding1, embedding2\n",
    "\n",
    "def get_embedding(model, text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def model_similarity(model, a, b):\n",
    "    return model.similarity(a, b)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "#def create_pairs(texts, authors, n_pairs):\n",
    "#    pairs = []\n",
    "#    labels = []\n",
    "#    for _ in range(n_pairs):\n",
    "#        idx1, idx2 = np.random.choice(len(texts), 2, replace=False)\n",
    "#        pairs.append((texts[idx1], texts[idx2]))\n",
    "#        labels.append(int(authors[idx1] == authors[idx2]))\n",
    "#    return pairs, labels\n",
    "\n",
    "def create_pairs(texts, authors, n_pairs):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    unique_authors = list(set(authors))\n",
    "    e = 0\n",
    "    t = 0\n",
    "    \n",
    "    # Create positive pairs (same author)\n",
    "    n_positive = n_pairs // 2\n",
    "    print(n_positive)\n",
    "    while t < n_positive and e < 50:\n",
    "        author = np.random.choice(unique_authors)\n",
    "        author_texts = [text for text, a in zip(texts, authors) if a == author]\n",
    "        if len(author_texts) < 2:\n",
    "            e+=1\n",
    "            print(\"e: \", e)\n",
    "            continue\n",
    "        text1, text2 = np.random.choice(author_texts, 2, replace=False)\n",
    "        pairs.append((text1, text2))\n",
    "        labels.append(1)\n",
    "        t+=1\n",
    "        print(\"t: \", t)\n",
    "        e=0\n",
    "    \n",
    "    # Create negative pairs (different authors)\n",
    "    n_negative = n_pairs - len(pairs)\n",
    "    for _ in range(n_negative):\n",
    "        author1, author2 = np.random.choice(unique_authors, 2, replace=False)\n",
    "        text1 = np.random.choice([text for text, a in zip(texts, authors) if a == author1])\n",
    "        text2 = np.random.choice([text for text, a in zip(texts, authors) if a == author2])\n",
    "        pairs.append((text1, text2))\n",
    "        labels.append(0)\n",
    "    \n",
    "    return pairs, labels\n",
    "\n",
    "def predict_authorship(model, tokenizer, text1, text2, device):\n",
    "    model.eval()\n",
    "    encoding1 = tokenizer.encode_plus(\n",
    "        text1,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoding2 = tokenizer.encode_plus(\n",
    "        text2,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids1 = encoding1['input_ids'].to(device)\n",
    "    attention_mask1 = encoding1['attention_mask'].to(device)\n",
    "    input_ids2 = encoding2['input_ids'].to(device)\n",
    "    attention_mask2 = encoding2['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding1, embedding2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        similarity = nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def validate_similarities(model, tokenizer, pairs, device):\n",
    "    similarities = []\n",
    "    for text1, text2 in tqdm(pairs, desc=\"Computing similarities\"):\n",
    "        similarity = predict_authorship(model, tokenizer, text1, text2, device)\n",
    "        centered_similarity = 0.5 * (similarity + 1)\n",
    "        #similarity = model_similarity(model, emb1, emb2)\n",
    "        similarities.append(centered_similarity)\n",
    "    #softsims = softmax(similarities)\n",
    "    return similarities\n",
    "\n",
    "def validate_threshold(labels, threshold, similarities):\n",
    "    predictions = [int(sim > threshold) for sim in similarities]\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return accuracy, f1\n",
    "\n",
    "def main():\n",
    "    # Load Reddit dataset\n",
    "    data = load_dataset(\"reddit\", split=\"train[10000:100000]\", trust_remote_code=True)\n",
    "    texts = data['content']\n",
    "    authors = data['author']\n",
    "\n",
    "    import gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    #load tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    #Load embedding model\n",
    "    #model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    model = ContrastiveAuthorshipModel().to(device)\n",
    "    model.load_state_dict(torch.load('best_authorship_model.pth'))\n",
    "\n",
    "    # Create pairs for validation\n",
    "    pairs, labels = create_pairs(texts, authors, n_pairs=200)\n",
    "\n",
    "    # Validate thresholds\n",
    "    thresholds = np.arange(0.01, 0.8, 0.02)\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    similarities = validate_similarities(model, tokenizer, pairs, device)\n",
    "    for threshold in thresholds:\n",
    "        print(f\"Validating threshold: {threshold}\")\n",
    "        accuracy, f1 = validate_threshold(labels, threshold, similarities)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "        print(f\"Accuracy: {accuracy}, F1 Score: {f1}\")\n",
    "\n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, accuracies, label='Accuracy', marker='o')\n",
    "    plt.plot(thresholds, f1_scores, label='F1 Score', marker='s')\n",
    "    plt.xlabel('Similarity Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Accuracy and F1 Score vs Similarity Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('validation_results.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Find best threshold\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(f\"Best F1 Score: {max(f1_scores)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "class HybridAuthorshipModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='distilbert-base-uncased', ngram_range=(1, 3), max_features=5000):\n",
    "        super().__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(pretrained_model_name)\n",
    "        self.projection = nn.Linear(self.distilbert.config.hidden_size, 128)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # N-gram vectorizer\n",
    "        self.ngram_vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
    "        \n",
    "        # Combine embeddings and n-gram features\n",
    "        self.combine_layer = nn.Linear(128 + max_features, 64)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, ngram_features1, ngram_features2):\n",
    "        # Process through DistilBERT\n",
    "        output1 = self.distilbert(input_ids1, attention_mask=attention_mask1)\n",
    "        output2 = self.distilbert(input_ids2, attention_mask=attention_mask2)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedding1 = self.projection(self.dropout(output1.last_hidden_state[:, 0, :]))\n",
    "        embedding2 = self.projection(self.dropout(output2.last_hidden_state[:, 0, :]))\n",
    "        \n",
    "        # Combine with n-gram features\n",
    "        combined1 = torch.cat([embedding1, ngram_features1], dim=1)\n",
    "        combined2 = torch.cat([embedding2, ngram_features2], dim=1)\n",
    "        \n",
    "        # Process through combination layers\n",
    "        processed1 = self.combine_layer(combined1)\n",
    "        processed2 = self.combine_layer(combined2)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity = torch.cosine_similarity(processed1, processed2)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "def prepare_ngram_features(texts, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "        vectorizer.fit(texts)\n",
    "    \n",
    "    features = vectorizer.transform(texts).toarray()\n",
    "    features_normalized = normalize(features, norm='l2', axis=1)\n",
    "    return torch.FloatTensor(features_normalized), vectorizer\n",
    "\n",
    "def contrastive_loss(similarity, label, margin=1.0):\n",
    "    loss_same = label * torch.pow(1 - similarity, 2)\n",
    "    loss_diff = (1 - label) * torch.pow(torch.clamp(similarity - margin, min=0.0), 2)\n",
    "    return (loss_same + loss_diff).mean()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        ngram_features1 = batch['ngram_features1'].to(device)\n",
    "        ngram_features2 = batch['ngram_features2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        similarity = model(input_ids1, attention_mask1, input_ids2, attention_mask2, ngram_features1, ngram_features2)\n",
    "        loss = contrastive_loss(similarity, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Load your dataset\n",
    "    texts, authors = load_dataset()  # Implement this function to load your data\n",
    "    \n",
    "    # Prepare n-gram features\n",
    "    ngram_features, vectorizer = prepare_ngram_features(texts)\n",
    "    \n",
    "    # Create dataset and data loader (you'll need to implement these)\n",
    "    train_loader = create_data_loader(texts, authors, ngram_features)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = HybridAuthorshipModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'hybrid_authorship_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mcant\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\mcant\\OneDrive\\Documents\\GitHub\\Shared-Specific-Distinctive-Tendencies\\venv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:  14%|█▎        | 43/313 [00:26<02:40,  1.68it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class HybridAuthorshipModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='distilbert-base-uncased', ngram_range=(1, 3), max_features=5000):\n",
    "        super().__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(pretrained_model_name)\n",
    "        self.projection = nn.Linear(self.distilbert.config.hidden_size, 128)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # N-gram vectorizer\n",
    "        self.ngram_vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
    "        \n",
    "        # Combine embeddings and n-gram features\n",
    "        self.combine_layer = nn.Linear(128 + max_features, 64)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, ngram_features1, ngram_features2):\n",
    "        # Process through DistilBERT\n",
    "        output1 = self.distilbert(input_ids1, attention_mask=attention_mask1)\n",
    "        output2 = self.distilbert(input_ids2, attention_mask=attention_mask2)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embedding1 = self.projection(self.dropout(output1.last_hidden_state[:, 0, :]))\n",
    "        embedding2 = self.projection(self.dropout(output2.last_hidden_state[:, 0, :]))\n",
    "        \n",
    "        # Combine with n-gram features\n",
    "        combined1 = torch.cat([embedding1, ngram_features1], dim=1)\n",
    "        combined2 = torch.cat([embedding2, ngram_features2], dim=1)\n",
    "        \n",
    "        # Process through combination layers\n",
    "        processed1 = self.combine_layer(combined1)\n",
    "        processed2 = self.combine_layer(combined2)\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarity = torch.cosine_similarity(processed1, processed2)\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "def prepare_ngram_features(texts, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "        vectorizer.fit(texts)\n",
    "    \n",
    "    features = vectorizer.transform(texts).toarray()\n",
    "    features_normalized = normalize(features, norm='l2', axis=1)\n",
    "    return torch.FloatTensor(features_normalized), vectorizer\n",
    "\n",
    "def contrastive_loss(similarity, label):\n",
    "    loss_same = label * torch.pow(1 - similarity, 2)\n",
    "    loss_diff = (1 - label) * torch.pow(torch.clamp(similarity, min=0.0), 2)\n",
    "    return (loss_same + loss_diff).mean()\n",
    "\n",
    "class RedditAuthorshipDataset(Dataset):\n",
    "    def __init__(self, texts, authors, ngram_features, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.authors = authors\n",
    "        self.ngram_features = ngram_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text1, author1 = self.texts[idx], self.authors[idx]\n",
    "        other_idx = np.random.randint(len(self.texts))\n",
    "        text2, author2 = self.texts[other_idx], self.authors[other_idx]\n",
    "        \n",
    "        label = 1 if author1 == author2 else 0\n",
    "\n",
    "        encoding1 = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer.encode_plus(\n",
    "            text2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids1': encoding1['input_ids'].flatten(),\n",
    "            'attention_mask1': encoding1['attention_mask'].flatten(),\n",
    "            'input_ids2': encoding2['input_ids'].flatten(),\n",
    "            'attention_mask2': encoding2['attention_mask'].flatten(),\n",
    "            'ngram_features1': self.ngram_features[idx],\n",
    "            'ngram_features2': self.ngram_features[other_idx],\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def pull_dataset(num_samples=10000):\n",
    "    # Load Reddit dataset\n",
    "    data = load_dataset(\"reddit\", split=f\"train[:{num_samples}]\", trust_remote_code=True)\n",
    "    texts = data['content']\n",
    "    authors = data['author']\n",
    "    return texts, authors\n",
    "\n",
    "def create_data_loader(texts, authors, ngram_features, tokenizer, batch_size=32):\n",
    "    dataset = RedditAuthorshipDataset(texts, authors, ngram_features, tokenizer)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        ngram_features1 = batch['ngram_features1'].to(device)\n",
    "        ngram_features2 = batch['ngram_features2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        similarity = model(input_ids1, attention_mask1, input_ids2, attention_mask2, ngram_features1, ngram_features2)\n",
    "        loss = contrastive_loss(similarity, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load dataset\n",
    "    texts, authors = pull_dataset(num_samples=10000)  # Adjust number of samples as needed\n",
    "    \n",
    "    # Prepare n-gram features\n",
    "    ngram_features, vectorizer = prepare_ngram_features(texts)\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Create data loader\n",
    "    train_loader = create_data_loader(texts, authors, ngram_features, tokenizer)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = HybridAuthorshipModel().to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'hybrid_authorship_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to determine authorship?\n",
    "# 1. Writing Style: Analyze the overall writing style, including sentence structure, vocabulary, and tone. Consistent use of certain phrases or idioms can indicate the same author.\n",
    "    - Transformers\n",
    "# 2. Punctuation and Grammar: Look for unique punctuation habits or grammatical structures. Authors often have consistent patterns in their use of commas, semicolons, and other punctuation marks.\n",
    "    - Transformers\n",
    "# 3. Spelling and Typos: Check for consistent spelling choices, especially with words that have multiple correct spellings (e.g., \"color\" vs. \"colour\"). Frequent typos or misspellings can also be a clue.\n",
    "    - Transformers, Ngram, Stylometrics\n",
    "# 4. Subject Matter: Consider the topics and themes discussed in the texts. Authors often write about subjects they are familiar with or passionate about.\n",
    "    - Transformers\n",
    "# 5. Use of Technical Terms: Identify any specialized vocabulary or jargon. Consistent use of specific terms can suggest the same author, especially in technical or niche topics.\n",
    "    - Transformers, Ngram, Stylometrics\n",
    "# 6. Formatting and Structure: Observe the formatting style, such as paragraph length, use of bullet points, or headings. Consistent structural elements can be indicative of the same author.\n",
    "    - Transformers, Stylometrics\n",
    "# 7. Emotional Tone: Evaluate the emotional tone or sentiment expressed in the texts. Authors may have a characteristic way of expressing emotions or attitudes.\n",
    "    - Transformers, Sentiment Analysis\n",
    "# 8. Use of Metaphors and Analogies: Look for unique metaphors or analogies. Authors often have a distinctive way of illustrating their points.\n",
    "    - Transformers, Stylometrics\n",
    "# 9. Frequency of Passive vs. Active Voice: Analyze the preference for passive or active voice. Some authors consistently use one over the other.\n",
    "    - Transformers, Ngram, Stylometrics\n",
    "# 10. Consistency in Perspective: Check for consistency in narrative perspective (first-person, second-person, third-person) and point of view.\n",
    "    - Transformers, Stylometrics\n",
    "\n",
    "Keywords, Keyphrases, Grammar, Tone, Syntax, Punctuation, Spelling, Vocabulary, Frequency, Position, Sentence Structure, Paragraph Length, Paragraph Structure, Narrative, Perspective, Dialect, Rhetorical Devices, Sentiment, Topic, Theme + When/Where/How\n",
    "\n",
    "# Components that make someone's message style unique:\n",
    "# 1. Writing Style: The overall structure, vocabulary, and tone used in writing.\n",
    "# 2. Punctuation and Grammar: Unique habits in punctuation and grammatical structures.\n",
    "# 3. Spelling and Typos: Consistent spelling choices and frequent typos.\n",
    "# 4. Subject Matter: Topics and themes frequently discussed.\n",
    "# 5. Use of Technical Terms: Specialized vocabulary or jargon.\n",
    "# 6. Formatting and Structure: Paragraph length, use of bullet points, or headings.\n",
    "# 7. Emotional Tone: The sentiment or emotional tone expressed.\n",
    "# 8. Use of Metaphors and Analogies: Unique ways of illustrating points.\n",
    "# 9. Frequency of Passive vs. Active Voice: Preference for passive or active voice.\n",
    "# 10. Consistency in Perspective: Narrative perspective and point of view.\n",
    "# 11. Keywords and Keyphrases: Specific words or phrases frequently used.\n",
    "# 12. Syntax: Sentence structure and complexity.\n",
    "# 13. Dialect: Regional language variations or slang.\n",
    "# 14. Rhetorical Devices: Use of rhetorical questions, repetition, etc.\n",
    "# 15. Sentiment: Overall sentiment or mood conveyed.\n",
    "# 16. Topic Relevance: How topics are connected to time, place, or method.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
